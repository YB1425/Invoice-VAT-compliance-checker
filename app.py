import streamlit as st
import pandas as pd
import requests
import time
import datetime
from io import BytesIO

# ==== CONFIG ====
st.set_page_config(page_title="Invoice Compliance Checker", layout="centered")

MAX_MB = 75
MAX_BYTES = MAX_MB * 1024 * 1024
MAX_FILES = 8

INSTANCE = st.secrets["DATABRICKS_INSTANCE"]
TOKEN = st.secrets["DATABRICKS_TOKEN"]
VOLUME_PATH = st.secrets["VOLUME_PATH"]
ARCHIVE_PATH = st.secrets["ARCHIVE_PATH"]
JOB_ID = st.secrets["JOB_ID"]
WAREHOUSE_ID = st.secrets["WAREHOUSE_ID"]

MAIN_PASSWORD = st.secrets["MAIN_PASSWORD"]
FINANCE_PASSWORD = st.secrets["FINANCE_PASSWORD"]

headers = {"Authorization": f"Bearer {TOKEN}"}

# ==== SESSION STATE ====
if "role" not in st.session_state:
    st.session_state.role = None  # "main" or "finance"

if "language" not in st.session_state:
    st.session_state.language = "en"  # default English

# ==== LANGUAGE STRINGS ====
STRINGS = {
    "en": {
        "title": "ðŸš— Invoice Compliance Checker",
        "main_tab": "ðŸ“¥ New Compliance Check",
        "inv_tab": "ðŸ“‚ Archived Invoices",
        "fail_tab": "ðŸ“‚ Archived Failed Checks",
        "password_prompt": "ðŸ”‘ Enter password to access this section",
        "finance_prompt": "Finance-only access. Please enter the finance password in Tab 1.",
        "logout": "ðŸšª Logout",
        "batch_name": "ðŸ“¦ Enter a batch name (optional)",
        "upload_label": "Upload up to 8 invoice PDFs",
        "received": "Received {n} file(s).",
        "too_big": "{n} file(s) exceed {mb} MB and were skipped: {files}",
        "run_check": "ðŸš€ Run VAT Compliance Check",
        "summary": "ðŸ“„ Invoice Summary",
        "failed": "âš ï¸ Failed Checks",
        "all_passed": "ðŸŽ‰ All invoices passed compliance checks!",
        "export": "ðŸ“¥ Export Results",
        "download_excel": "â¬‡ï¸ Download Excel",
        "download_inv_csv": "â¬‡ï¸ Download Invoices Archive CSV",
        "download_fail_csv": "â¬‡ï¸ Download Checks Archive CSV",
        "no_archives": "No archived data found yet.",
        "connection_ok": "âœ… SQL Warehouse connected! Today's date = {date}",
        "connection_fail": "âŒ SQL Warehouse test failed.",
        "wrong_password": "âŒ Incorrect password. Please try again.",
        "disclaimer": """
---
âš ï¸ **Disclaimer:**  
This program is a **proof-of-concept tool**.  
- Results may be inaccurate or incomplete.  
- It does **not** validate electronic VAT **QR codes** or **UBL XML** compliance.  
For official ZATCA compliance, always use certified solutions.
"""
    },
    "ar": {
        "title": "ðŸš— Ø£Ø¯Ø§Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„ÙÙˆØ§ØªÙŠØ±",
        "main_tab": "ðŸ“¥ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙÙˆØ§ØªÙŠØ± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©",
        "inv_tab": "ðŸ“‚ Ø§Ù„ÙÙˆØ§ØªÙŠØ± Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©",
        "fail_tab": "ðŸ“‚ Ø§Ù„Ø¥Ø®ÙØ§Ù‚Ø§Øª Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©",
        "password_prompt": "ðŸ”‘ Ø£Ø¯Ø®Ù„ ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ± Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù…",
        "finance_prompt": "Ø§Ù„ÙˆØµÙˆÙ„ Ø®Ø§Øµ Ø¨Ù‚Ø³Ù… Ø§Ù„Ù…Ø§Ù„ÙŠØ©. ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ ÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ± Ø§Ù„Ù…Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„ØªØ¨ÙˆÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„.",
        "logout": "ðŸšª ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø±ÙˆØ¬",
        "batch_name": "ðŸ“¦ Ø£Ø¯Ø®Ù„ Ø§Ø³Ù… Ø§Ù„Ø¯ÙØ¹Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)",
        "upload_label": "Ù‚Ù… Ø¨Ø±ÙØ¹ Ù…Ø§ ÙŠØµÙ„ Ø¥Ù„Ù‰ 8 Ù…Ù„ÙØ§Øª PDF Ù„Ù„ÙÙˆØ§ØªÙŠØ±",
        "received": "ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… {n} Ù…Ù„Ù(Ø§Øª).",
        "too_big": "{n} Ù…Ù„Ù(Ø§Øª) ØªØªØ¬Ø§ÙˆØ² {mb} Ù…ÙŠØºØ§Ø¨Ø§ÙŠØª ÙˆØªÙ… ØªØ¬Ø§Ù‡Ù„Ù‡Ø§: {files}",
        "run_check": "ðŸš€ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¶Ø±ÙŠØ¨Ø© Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø¶Ø§ÙØ©",
        "summary": "ðŸ“„ Ù…Ù„Ø®Øµ Ø§Ù„ÙÙˆØ§ØªÙŠØ±",
        "failed": "âš ï¸ Ø§Ù„ÙØ­ÙˆØµØ§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©",
        "all_passed": "ðŸŽ‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙÙˆØ§ØªÙŠØ± Ø§Ø¬ØªØ§Ø²Øª Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©!",
        "export": "ðŸ“¥ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬",
        "download_excel": "â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ù…Ù„Ù Excel",
        "download_inv_csv": "â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ø£Ø±Ø´ÙŠÙ Ø§Ù„ÙÙˆØ§ØªÙŠØ± CSV",
        "download_fail_csv": "â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ø£Ø±Ø´ÙŠÙ Ø§Ù„Ø¥Ø®ÙØ§Ù‚Ø§Øª CSV",
        "no_archives": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¤Ø±Ø´ÙØ© Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†.",
        "connection_ok": "âœ… ØªÙ… Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ SQL Warehouse! ØªØ§Ø±ÙŠØ® Ø§Ù„ÙŠÙˆÙ… = {date}",
        "connection_fail": "âŒ ÙØ´Ù„ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ SQL Warehouse.",
        "wrong_password": "âŒ ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ± ØºÙŠØ± ØµØ­ÙŠØ­Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.",
        "disclaimer": """
---
âš ï¸ **ØªÙ†ÙˆÙŠÙ‡:**  
Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø© Ù…Ø¬Ø±Ø¯ **Ø¥Ø«Ø¨Ø§Øª Ù…ÙÙ‡ÙˆÙ…**.  
- Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØºÙŠØ± Ø¯Ù‚ÙŠÙ‚Ø© Ø£Ùˆ ØºÙŠØ± ÙƒØ§Ù…Ù„Ø©.  
- Ù„Ø§ ØªØªØ­Ù‚Ù‚ Ù…Ù† **Ø±Ù…ÙˆØ² QR Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ©** Ø£Ùˆ **UBL XML** Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø¶Ø±ÙŠØ¨Ø© Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø¶Ø§ÙØ©.  
Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø±Ø³Ù…ÙŠØ© Ù…Ø¹ Ù‡ÙŠØ¦Ø© Ø§Ù„Ø²ÙƒØ§Ø© ÙˆØ§Ù„Ø¶Ø±ÙŠØ¨Ø© ÙˆØ§Ù„Ø¬Ù…Ø§Ø±ÙƒØŒ ÙŠØ±Ø¬Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯Ø©.
"""
    }
}

# ==== LANG SELECTOR + SIDEBAR LOGOUT ====
with st.sidebar:
    lang = st.radio("ðŸŒ Language / Ø§Ù„Ù„ØºØ©", ["English", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"])
    st.session_state.language = "ar" if lang == "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" else "en"

    if st.session_state.role:
        st.success(f"Logged in as: {st.session_state.role}")
        if st.button(STRINGS[st.session_state.language]["logout"]):
            st.session_state.role = None
            st.rerun()

T = STRINGS[st.session_state.language]

st.title(T["title"])

# ==== HELPERS ====
@st.cache_data(ttl=60)
def run_sql(sql: str):
    submit_url = f"{INSTANCE}/api/2.0/sql/statements/"
    payload = {"statement": sql, "warehouse_id": WAREHOUSE_ID, "wait_timeout": "30s"}
    resp = requests.post(submit_url, headers=headers, json=payload).json()
    if "statement_id" not in resp:
        return pd.DataFrame()
    statement_id = resp["statement_id"]

    while True:
        res = requests.get(f"{submit_url}{statement_id}", headers=headers).json()
        if res["status"]["state"] in ["SUCCEEDED", "FAILED", "CANCELED"]:
            break
        time.sleep(2)

    if res["status"]["state"] != "SUCCEEDED":
        return pd.DataFrame()
    if "result" not in res or "data_array" not in res["result"]:
        return pd.DataFrame()

    cols = [c["name"] for c in res["manifest"]["schema"]["columns"]]
    rows = []
    for r in res["result"]["data_array"]:
        row = []
        for c in r:
            row.append(c.get("value") if isinstance(c, dict) else c)
        rows.append(row)
    return pd.DataFrame(rows, columns=cols)

def upload_to_volume(file_name, file_bytes, dest_path):
    url = f"{INSTANCE}/api/2.0/fs/files{dest_path}/{file_name}"
    resp = requests.put(url, headers=headers, data=file_bytes)
    resp.raise_for_status()

def run_parse_job():
    url = f"{INSTANCE}/api/2.1/jobs/run-now"
    resp = requests.post(url, headers=headers, json={"job_id": JOB_ID})
    resp.raise_for_status()
    return resp.json()["run_id"]

def wait_for_result(run_id):
    url = f"{INSTANCE}/api/2.1/jobs/runs/get?run_id={run_id}"
    while True:
        resp = requests.get(url, headers=headers).json()
        if resp["state"]["life_cycle_state"] == "TERMINATED":
            return resp
        time.sleep(5)

def df_to_excel(df_dict):
    output = BytesIO()
    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
        for sheet, df in df_dict.items():
            df.to_excel(writer, sheet_name=sheet, index=False)
    return output.getvalue()

def cleanup_volume(path, batch_name):
    # Delete the whole batch folder in one request
    folder_path = f"{path}/{batch_name}"
    url = f"{INSTANCE}/api/2.0/fs/files{folder_path}?recursive=true"
    resp = requests.delete(url, headers=headers)
    resp.raise_for_status()
    return f"Deleted folder {folder_path}"
# ==== TABS ====
tab1, tab2, tab3 = st.tabs([T["main_tab"], T["inv_tab"], T["fail_tab"]])

# --- Main Tab ---
with tab1:
    if st.session_state.role not in ["main", "finance"]:
        pw = st.text_input(T["password_prompt"], type="password", key="main_pw")
        if pw:
            if pw == MAIN_PASSWORD:
                st.session_state.role = "main"
                st.success("Access granted âœ…")
            elif pw == FINANCE_PASSWORD:
                st.session_state.role = "finance"
                st.success("Finance access granted âœ…")
            else:
                st.error(T["wrong_password"])
                st.stop()
        else:
            st.stop()
    batch_name_input = st.text_input(T["batch_name"], placeholder="e.g. Sept14_Invoices")
    if batch_name_input and batch_name_input.strip():
        BATCH_NAME = batch_name_input.strip().replace(" ", "_")
    else:
        BATCH_NAME = datetime.datetime.now(datetime.UTC).strftime("%Y%m%d_%H%M%S")

    uploads = st.file_uploader(T["upload_label"], type=["pdf"], accept_multiple_files=True)

    if uploads:
        if len(uploads) > MAX_FILES:
            st.error(f"âš ï¸ You can only upload up to {MAX_FILES} files at once.")
        else:
            too_big = [f for f in uploads if f.size > MAX_BYTES]
            ok = [f for f in uploads if f.size <= MAX_BYTES]

            if too_big:
                st.error(T["too_big"].format(n=len(too_big), mb=MAX_MB, files=", ".join(f.name for f in too_big)))

            if ok:
                st.success(T["received"].format(n=len(ok)))
                st.dataframe(pd.DataFrame(
                    [{"File": f.name, "Size (MB)": round(f.size / 1024 / 1024, 2)} for f in ok]
                ))

                if st.button(T["run_check"]):
                    # Upload files (working + archive immediately)
                    with st.spinner("Uploading files..."):
                        for f in ok:
                            file_bytes = f.read()
                            upload_to_volume(f.name, file_bytes, f"{VOLUME_PATH}/{BATCH_NAME}")   # working
                            upload_to_volume(f.name, file_bytes, f"{ARCHIVE_PATH}/{BATCH_NAME}")  # archive

                    # Run job
                    with st.spinner("Running Databricks job..."):
                        run_id = run_parse_job()
                        wait_for_result(run_id)

                    st.success("âœ… Job completed! Fetching results...")

                    # --- Summary ---
                    df_summary = run_sql("""
                        SELECT path, invoice_number, issue_date, final_decision
                        FROM dev_uc_catalog.default.zatca_invoices_head
                        ORDER BY path
                    """)
                    st.subheader(T["summary"])
                    st.dataframe(df_summary)

                    # --- Failed checks ---
                    df_details = run_sql("""
                        SELECT h.path, h.invoice_number, h.issue_date, h.final_decision,
                               c.id AS failed_rule_id, c.name AS failed_rule_name, c.reason AS failed_reason
                        FROM dev_uc_catalog.default.zatca_invoices_head h
                        JOIN dev_uc_catalog.default.zatca_checks_flat c
                          ON h.path = c.path
                        WHERE c.result = 'fail'
                        ORDER BY h.path, c.id
                    """)
                    if not df_details.empty:
                        st.subheader(T["failed"])
                        st.dataframe(df_details)
                    else:
                        st.success(T["all_passed"])

                    # --- Export buttons ---
                    st.subheader(T["export"])
                    excel_data = df_to_excel({"Summary": df_summary, "Failed Checks": df_details})
                    st.download_button(T["download_excel"],
                                       data=excel_data,
                                       file_name=f"vat_compliance_results_{BATCH_NAME}.xlsx")

                    # Archive & reset DB
                    run_sql(f"""
                        INSERT INTO dev_uc_catalog.default.zatca_invoices_head_archive
                        SELECT *, '{BATCH_NAME}' AS batch_name
                        FROM dev_uc_catalog.default.zatca_invoices_head
                    """)
                    run_sql(f"""
                        INSERT INTO dev_uc_catalog.default.zatca_checks_flat_archive
                        SELECT *, '{BATCH_NAME}' AS batch_name
                        FROM dev_uc_catalog.default.zatca_checks_flat
                    """)
                    run_sql("TRUNCATE TABLE dev_uc_catalog.default.zatca_invoices_head")
                    run_sql("TRUNCATE TABLE dev_uc_catalog.default.zatca_checks_flat")
                    run_sql("TRUNCATE TABLE dev_uc_catalog.default.zatca_invoice_check_parsed")

                    msg = cleanup_volume(VOLUME_PATH, BATCH_NAME)
                    st.success(f"Session archived and reset âœ… ({msg})")

# --- Archived Invoices (Finance only) ---
with tab2:
    if st.session_state.role != "finance":
        st.warning(T["finance_prompt"])
        st.stop()

    st.subheader(T["inv_tab"])
    batch_list = run_sql("SELECT DISTINCT batch_name FROM dev_uc_catalog.default.zatca_invoices_head_archive ORDER BY batch_name DESC")
    if not batch_list.empty:
        selected_batch = st.selectbox("Choose a batch", batch_list["batch_name"], key="batch_invoices")
        df_archive_invoices = run_sql(f"""
            SELECT * FROM dev_uc_catalog.default.zatca_invoices_head_archive
            WHERE batch_name = '{selected_batch}'
            ORDER BY path
        """)
        st.dataframe(df_archive_invoices)
        st.download_button(T["download_inv_csv"],
                           data=df_archive_invoices.to_csv(index=False).encode("utf-8"),
                           file_name=f"invoices_{selected_batch}.csv",
                           mime="text/csv")
    else:
        st.info(T["no_archives"])

# --- Archived Failed Checks (Finance only) ---
with tab3:
    if st.session_state.role != "finance":
        st.warning(T["finance_prompt"])
        st.stop()

    st.subheader(T["fail_tab"])
    batch_list = run_sql("SELECT DISTINCT batch_name FROM dev_uc_catalog.default.zatca_checks_flat_archive ORDER BY batch_name DESC")
    if not batch_list.empty:
        selected_batch = st.selectbox("Choose a batch", batch_list["batch_name"], key="batch_checks")
        df_archive_checks = run_sql(f"""
            SELECT * FROM dev_uc_catalog.default.zatca_checks_flat_archive
            WHERE batch_name = '{selected_batch}'
            ORDER BY path, id
        """)
        st.dataframe(df_archive_checks)
        st.download_button(T["download_fail_csv"],
                           data=df_archive_checks.to_csv(index=False).encode("utf-8"),
                           file_name=f"checks_{selected_batch}.csv",
                           mime="text/csv")
    else:
        st.info(T["no_archives"])

# ==== DISCLAIMER ====
st.markdown(T["disclaimer"])
